
\documentclass[%
pdf,
%nocolorBG,
colorBG,
slideColor,
tcrico,
%slideBW,
%draft,
%frames
%azure
%contemporain
%nuancegris
%troispoints
%lignesbleues
%darkblue
%alienglow
%autumn
%default
%gyom
%blends
]{prosper}
\usepackage{amsmath}
%\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm2e}

\begin{document}

\begin{slide}{Knowledge Based Technologies}
	\textbf{Lecture 9} 
	\newline
	\textbf{An Introduction to Computational Learning Theory}

	\small
	John Moore \& Thomas Collins
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Computational Learning Theory }

What general laws constrain inductive learning?
\\

We seek theory to relate:
\begin{itemize}
	\item Probability of successful learning
	\item Number of training examples
	\item Complexity of hypothesis space
	\item Accuracy to which target concept is approximated
	\item Manner in which training examples presented
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Prototypical Concept Learning Task }
\tiny 
\begin{itemize} 
\item  {\bf Given:} 
	\begin{itemize}
	\item Instances $X$: Possible days, each described by the attributes {\em Sky, AirTemp, Humidity, Wind, Water, Forecast}
	\item Target function $c$: $EnjoySport: X \rightarrow \{0,1 \}$
 	\item Hypotheses $H$: Conjunctions of literals. E.g.\[\langle ?, Cold, High, ?, ?, ? \rangle. \]
	\item Training examples $D$: Positive and negative examples of the target function 
\[\langle x_1, c(x_1) \rangle , \ldots \langle x_m, c(x_m) \rangle\]
	\end{itemize}
\item {\bf Determine:}
	\begin{itemize}
	\item A hypothesis $h$ in $H$ such that $h(x)=c(x)$ for all $x$ in $D$?
	\item A hypothesis $h$ in $H$ such that $h(x)=c(x)$ for all $x$ in $X$?
	\end{itemize}
\end{itemize}
\end{slide}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Sample Complexity }
\tiny 

How many training examples are sufficient to learn the target concept?

\begin{enumerate}
\item
If learner proposes instances, as queries to teacher
\begin{itemize}
\item Learner proposes instance $x$, teacher provides $c(x)$
\end{itemize}

\item
If teacher (who knows $c$) provides training examples
\begin{itemize}
\item teacher provides sequence of examples of form $\langle x, c(x) \rangle$
\end{itemize}

\item
If some random process (e.g., nature) proposes instances
\begin{itemize}
\item instance $x$ generated randomly, teacher provides $c(x)$
\end{itemize}
\end{enumerate}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Sample Complexity: 1 }
\tiny 
Learner proposes instance $x$, teacher provides $c(x)$

(assume $c$ is in learner's hypothesis space $H$)


Optimal query strategy: play 20 questions
\begin{itemize} \item pick instance $x$ such that half of hypotheses in $VS$ classify $x$
positive, half classify $x$ negative
\item When this is possible, need $\lceil \log_2 |H| \rceil  $ queries to learn $c$
\item when not possible, need even more
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Sample Complexity: 2 }

Teacher (who knows $c$) provides training examples

(assume $c$ is in learner's hypothesis space $H$)


Optimal teaching strategy: depends on $H$ used by learner

Consider the case $H = $ conjunctions of up to $n$ boolean literals and their negations
\begin{itemize}
	\item[] e.g., $(AirTemp = Warm) \wedge (Wind = Strong)$, where $AirTemp, Wind,
	\ldots$ each have 2 possible values.
\end{itemize}
\begin{itemize} 
	\item if $n$ possible boolean attributes in $H$, $n + 1$ examples suffice
	\item why?
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Sample Complexity: 3 }
\tiny 
Given:
\begin{itemize}
\item set of instances $X$
\item set of hypotheses $H$
\item set of possible target concepts $C$
\item training instances generated by a fixed, unknown probability distribution $\cal{D}$ over $X$
\end{itemize}

Learner observes a sequence $D$ of training examples of form $\langle x, c(x) \rangle$, for some target concept $c \in C$
\begin{itemize}
\item instances $x$ are drawn from distribution $\cal{D}$
\item teacher provides target value $c(x)$ for each
\end{itemize}

Learner must output a hypothesis $h$ estimating $c$
\begin{itemize} 
\item $h$ is evaluated by its performance on subsequent instances drawn
according to $\cal{D}$
\end{itemize}

Note: randomly drawn instances, noise-free classifications
\end{slide}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{True Error of a Hypothesis }
\begin{itemize}
 \item Sample error estimates true error
 \item BUT we assumed $h$ is independent of the sample
\item However it's the sample that is used to learn $h$ --> strongly dependent
\end{itemize}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{True Error of a Hypothesis }
\tiny 
\begin{figure}
		\centering
		\includegraphics[scale=0.5]{./../bookps/pac-err.ps}
	\end{figure}

\begin{quote}
{\bf Definition:} 
\\ 
The {\bf true error} (denoted $error_{\cal{D}}(h)$) of
hypothesis $h$ with respect to target concept $c$ and distribution $\cal{D}$
is the probability that $h$ will misclassify an instance drawn at random
according to $\cal{D}$.
\[error_{\cal{D}}(h) \equiv \Pr_{x \in \cal{D}}[c(x) \neq h(x)] \]

\end{quote}
\end{slide}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Two Notions of Error }

{\em Training error} of hypothesis $h$ with respect to target concept $c$
\begin{itemize}
\item How often $h(x) \neq c(x)$ over training instances
\end{itemize}


{\em True error} of hypothesis $h$ with respect to $c$
\begin{itemize}
\item How often $h(x) \neq c(x)$ over future random instances
\end{itemize}


Our concern: 
\begin{itemize}
\item Can we bound the  true error of $h$ given the training error of
$h$?

\item First consider when training error of $h$ is zero (i.e., $h \in VS_{H,D}$)
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Exhausting the Version Space }
\tiny 

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{./../bookps/pac-vs-exhausted.ps}
\end{figure}

\centerline{($r =$ training error, $error = $ true error)}

\begin{quote}
{\bf Definition:} The version space $VS_{H,D}$ is said to be $\epsilon$-{\bf
 exhausted} with respect to $c$ and $\cal D$, if every hypothesis $h$ in
 $VS_{H,D}$ has error less than $\epsilon$ with respect to $c$ and $\cal D$.
\[(\forall h \in VS_{H,D})\  error_{\cal D}(h) < \epsilon \]
\end{quote}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{\normalsize How many examples will $\epsilon$-exhaust the VS? }
\tiny 
{\bf Theorem:} [Haussler, 1988].
\begin{quote}

If the hypothesis space $H$ is finite, and $D$ is a sequence of $m \geq 1$
independent random examples of some target concept $c$, then for any $0 \leq
\epsilon \leq 1$, the probability that the version space with respect to $H$
and $D$ is not $\epsilon$-exhausted (with respect to $c$) is less than \[|H|
e^{-\epsilon m} \]
\end{quote}

This bounds the probability that any consistent learner will output a hypothesis $h$ with $error(h) \geq \epsilon$

If we want to this probability to be below $\delta$ \[|H|e^{- \epsilon m} \leq \delta  \]

then

\[m \geq \frac{1}{\epsilon}(\ln|H| + \ln(1/\delta)) \]
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Learning Conjunctions of Boolean Literals }
\tiny 
How many examples are sufficient to assure with probability at 
least $(1 - \delta)$ that
\begin{itemize}
\item[] every $h$ in $VS_{H,D}$ satisfies $error_{\cal D}(h) \leq \epsilon$
\end{itemize}

\bigskip

Use our theorem:

\[m \geq \frac{1}{\epsilon}(\ln|H| + \ln(1/\delta)) \]

Suppose $H$ contains conjunctions of constraints on up to $n$ boolean
attributes (i.e., $n$ boolean literals).  Then $|H| = 3^n$, and

\[m \geq \frac{1}{\epsilon}(\ln 3^n + \ln(1/\delta)) \]

or

\[m \geq \frac{1}{\epsilon}(n \ln 3 + \ln(1/\delta)) \]
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{EnjoySport }
\tiny 

\[m \geq \frac{1}{\epsilon}(\ln|H| + \ln(1/\delta)) \]


If $H$ is as given in $EnjoySport$ then $|H| = 973$, and

\[m \geq \frac{1}{\epsilon}(\ln 973 + \ln(1/\delta)) \]

... if want to assure that with probability 95\%, $VS$ contains only
hypotheses with $error_{\cal D}(h) \leq .1$, then it is sufficient to have
$m$ examples, where

\[m \geq \frac{1}{.1}(\ln 973 + \ln(1/.05)) \]
\[m \geq 10 (\ln 973 + \ln 20) \]
\[m \geq 10 (6.88 + 3.00) \]
\[m \geq 98.8 \]
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{PAC Learning }

Consider a class $C$ of possible target concepts defined over a set of
instances $X$ of length $n$, and a learner $L$ using hypothesis space $H$.

\begin{quote}
{\em Definition:} $C$ is {\bf PAC-learnable} by $L$ using $H$ if for all $c
\in C$, distributions $\cal D$ over $X$, $\epsilon$ such that $0 < \epsilon <
1/2$, and $\delta$ such that $0 < \delta < 1/2$,

learner $L$ will with probability at least $(1-\delta)$ output a hypothesis $h
\in H$ such that $error_{\cal D}(h) \leq \epsilon$, in time that is polynomial
in $1/\epsilon$, $1/\delta$, $n$ and $size(c)$.
\end{quote}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Agnostic Learning }
\tiny
So far, assumed $c \in H$

\vspace*{.1in}

Agnostic learning setting: don't assume $c \in H$

\begin{itemize}
\item What do we want then?
\begin{itemize} \item The hypothesis $h$ that makes fewest errors on training data \end{itemize}
\item What is sample complexity in this case?
\end{itemize}

\[ m \geq \frac{1}{2 \epsilon^{2}}(\ln|H| + \ln(1/\delta)) \]
derived from Hoeffding bounds:
\[ Pr[error_{\cal D}(h) > error_D(h) + \epsilon] \leq e^{-2m\epsilon^{2}} \]
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Shattering a Set of Instances }

\begin{quote}
{\em Definition:} a {\bf dichotomy} of a set $S$ is a partition of $S$ into
two disjoint subsets.
\end{quote}

\begin{quote}
{\em Definition:} a set of instances $S$ is {\bf shattered} by hypothesis
space $H$ if and only if for every dichotomy of $S$ there exists some
hypothesis in $H$ consistent with this dichotomy.
\end{quote}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Three Instances Shattered }
	\begin{figure}
		\includegraphics[scale=1]{./../bookps/pac-shatter.ps}
	\end{figure}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{The Vapnik-Chervonenkis Dimension }


\begin{quote}
{\em Definition:} The {\bf Vapnik-Chervonenkis dimension}, $VC(H)$, of
hypothesis space $H$ defined over instance space $X$ is the size of the
largest finite subset of $X$ shattered by $H$.  If arbitrarily large finite
sets of $X$ can be shattered by $H$, then $VC(H) \equiv \infty$.
\end{quote}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{VC Dim. of Linear Decision Surfaces }

\begin{figure}		
		\includegraphics[scale=1]{./../bookps/pac-pts.ps}
	\end{figure}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Mistake Bounds }

So far: how many examples needed to learn?

\vspace*{.1in}
What about: how many mistakes before convergence?


\vspace*{.2in}

Let's consider similar setting to PAC learning:
\begin{itemize}
\item Instances drawn at random from $X$ according to distribution ${\cal D}$
\item Learner must classify each instance before receiving correct
classification from teacher
\item Can we bound the number of mistakes learner makes before converging?
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Mistake Bounds: Find-S }

Consider Find-S when $H=$ conjunction of boolean literals
\begin{quote}
\texttt{Find-S}:
\begin{itemize}
\item Initialize $h$ to the most specific hypothesis $l_{1} \wedge \neg l_{1} \wedge l_{2} \wedge \neg l_{2}
\ldots l_{n} \wedge \neg l_{n}$

\item For each positive training instance $x$
\begin{itemize}
\item Remove from $h$ any literal that is not satisfied by $x$
\end{itemize}
\item Output hypothesis $h$.
\end{itemize}
\end{quote}

How many mistakes before converging to correct $h$?
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Mistake Bounds: Find-S }


\begin{itemize}
\item $H$: conjunctions of n literals
\item Find-S never misclassifies a negative example
\item Only errors: + classified as -
\item Error at 1st iteration -> n of 2n literals eliminated
\item Remaining: at most n errors
\end{itemize}
\end{slide}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Mistake Bounds: Halving Algorithm }
\tiny 
Consider the Halving Algorithm:
\begin{itemize}
\item Learn concept using version space \texttt{Candidate-Elimination} algorithm
\item Classify new instances by majority vote of version space members
\end{itemize}

How many mistakes before converging to correct $h$?
\begin{itemize}
\item Mistake made when majority of $VC(H,D)$ misclassifies these are eliminated 
\item  $VC(H,D)$ is (at least) halved $log|H|4$ 
\item At most (worst case bound) 
\item May learn without making any mistakes (minority eliminated)
\end{itemize}
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{slide}{Optimal Mistake Bounds }
\tiny 
Let $M_{A}(C)$ be the max number of mistakes made by algorithm $A$ to learn
concepts in $C$.  (maximum over all possible $c \in C$, and all possible
training sequences)
\[M_{A}(C) \equiv \max_{c \in C} M_{A}(c)\]


{\em Definition:} Let $C$ be an arbitrary non-empty concept class.  The {\bf
optimal mistake bound} for $C$, denoted $Opt(C)$, is the minimum over all
possible learning algorithms $A$ of $M_{A}(C)$.
\[Opt(C) \equiv \min_{A \in learning\  algorithms} M_{A}(C) \]


\[  VC(C) \leq Opt(C) \leq M_{Halving}(C) \leq log_{2}(|C|). \]
\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{slide}{Summary}
\begin{itemize}
\item Computational Learning Theory: Related to the analysis of machine learning algorithms.
 \item  PAC model
\item  Sample complexity in PAC model result applies to any consistent learner with finite $H$
\item Agnostic learning
\item Complexity of $H: VC(H)$
\end{itemize}

\end{slide}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}




